{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the LinkedIn Jobs URL Structure\n",
    "\n",
    "https://www.linkedin.com/jobs/search/?keywords=python%20developer&location=San%20Francisco%20Bay%20Area&geoId=90000080&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\n",
    "\n",
    "\n",
    "This URL contains several parameters that we can use to customize our search, such as:\n",
    "\n",
    "keywords: The job title or skills you're searching for\n",
    "location: The geographic location where you want to search for jobs\n",
    "geoId: The unique identifier for the specified location\n",
    "position: The position of the job posting in the search results\n",
    "pageNum: The page number of the search results\n",
    "\n",
    "By modifying these parameters, we can scrape job postings for different job titles, locations, and pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Infinite Scrolling with Selenium\n",
    "One of the challenges when scraping LinkedIn job postings is that the website uses infinite scrolling. This means that as you scroll down the page, more job postings are loaded dynamically without changing the URL. To handle this, we'll be using Selenium, a web automation tool that allows us to control a web browser programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigating to the LinkedIn Jobs page: \n",
    "Using the driver.get() method to navigate to the LinkedIn Jobs page with your desired search parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.linkedin.com/jobs/search/?currentJobId=3882755006&geoId=115884833&keywords=python%20developer&location=Gurugram%2C%20Haryana%2C%20India&origin=JOB_SEARCH_PAGE_LOCATION_AUTOCOMPLETE&refresh=true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrolling to load more job postings:\n",
    "Use a loop to scroll down the page and load more job postings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code scrolls down the page until no more job postings are loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the job data \n",
    "\n",
    "Extracting Job Details from the Sidebar\n",
    "\n",
    "extracting the job data using BeautifulSoup or other parsing techniques.\n",
    "When you click on a job posting on LinkedIn, additional details about the job are displayed in a sidebar. To extract this information, will need to click on each job posting and scrape the data from the sidebar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all job posting elements:\n",
    "Use Selenium's find_elements_by_css_selector() method to find all the job posting elements on the page:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_listings = driver.find_elements_by_css_selector(\"div.job-card-container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through each job posting: Loop through each job posting element and click on it to open the sidebar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_listing in job_listings:\n",
    "    job_listing.click()\n",
    "    time.sleep(2)  # Wait for the sidebar to load\n",
    "    # Extract job details from the sidebar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract job details:\n",
    " Within the loop, we can use BeautifulSoup or other parsing techniques to extract the desired job details from the sidebar, such as job title, company name, location, salary range, and job description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the LinkedIn Job Scraper\n",
    "Now that we've covered the necessary setup and concepts, let's dive into the implementation of our LinkedIn job scraper. We'll be using the `requests` library to send HTTP requests and the `BeautifulSoup` library to parse the HTML responses.\n",
    "\n",
    "# Import the required libraries:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "which we alredy imporetd above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the base URL: We'll start by defining the base URL for the LinkedIn Jobs page, which includes the search parameters we're interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.linkedin.com/jobs/search/?currentJobId=3882755006&geoId=115884833&keywords=python%20developer&location=Gurugram%2C%20Haryana%2C%20India&origin=JOB_SEARCH_PAGE_LOCATION_AUTOCOMPLETE&refresh=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send the initial request:\n",
    " We'll send an initial request to the base URL to retrieve the first page of job postings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract job data: \n",
    "Using BeautifulSoup to find and extract the relevant job data from the HTML response. You can use CSS selectors or other techniques to locate the desired elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_listings = soup.find_all(\"div\", class_=\"job-card-container\")\n",
    "for job_listing in job_listings:\n",
    "    job_title = job_listing.find(\"h3\", class_=\"job-card-container__title\").text.strip()\n",
    "    company_name = job_listing.find(\"h4\", class_=\"job-card-container__company-name\").text.strip()\n",
    "    location = job_listing.find(\"span\", class_=\"job-card-container__location\").text.strip()\n",
    "    # Extract other job details as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paginate through the results:\n",
    "To scrape job postings from multiple pages, you'll need to modify the pageNum parameter in the URL and send additional requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_num = 0\n",
    "while True:\n",
    "    url = f\"{base_url}&pageNum={page_num}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    job_listings = soup.find_all(\"div\", class_=\"job-card-container\")\n",
    "    if not job_listings:\n",
    "        break  # No more job postings\n",
    "    for job_listing in job_listings:\n",
    "        # Extract job data\n",
    "        # ...\n",
    "        page_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code sends a request to the next page by incrementing the pageNum parameter until no more job postings are found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Scraper and Saving Data\n",
    "\n",
    "After implementing the scraper, can run it and save the scraped data to a file or database for further analysis or processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a list or dictionary to store the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append the scraped data to the list or dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_listing in job_listings:\n",
    "    job_title = job_listing.find(\"h3\", class_=\"job-card-container__title\").text.strip()\n",
    "    company_name = job_listing.find(\"h4\", class_=\"job-card-container__company-name\").text.strip()\n",
    "    location = job_listing.find(\"span\", class_=\"job-card-container__location\").text.strip()\n",
    "    job_data.append({\n",
    "        \"job_title\": job_title,\n",
    "        \"company_name\": company_name,\n",
    "        \"location\": location,\n",
    "        # Add other job details as needed\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data to a file or database: \n",
    " can use libraries like pandas or csv to save the data to a CSV file, or use a database library like sqlite3 or pymongo to store the data in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save data to a CSV file\n",
    "df = pd.DataFrame(job_data)\n",
    "df.to_csv(\"linkedin_jobs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
